# -*- coding: utf-8 -*-
"""Thyroid Disease Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v-PFTSEhN-Na4tlX8JJfKvkXmooDMplh
"""

import pandas as pd
import numpy as np
import time
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from xgboost import XGBClassifier
import joblib

# Step 1: Data Collection & Preprocessing
# Load dataset
csv_file_path = "/content/hypothyroid.csv"
df = pd.read_csv(csv_file_path)

# Conduct Exploratory Data Analysis (EDA)
print("Dataset Overview:")
print(df.head())
print("\nData Info:")
print(df.info())
print("\nMissing Values Per Column:")
print(df.isnull().sum())
print("\nStatistical Summary of Numeric Columns:")
print(df.describe())

import numpy as np
import pandas as pd

# Step 1: Remove rows containing "?" values by replacing with NaN and dropping them
df.replace("?", np.nan, inplace=True)
df.dropna(inplace=True)

# Step 2: Define binary columns
binary_cols = ['on thyroxine', 'query on thyroxine', 'on antithyroid medication',
               'sick', 'pregnant', 'thyroid surgery', 'I131 treatment',
               'query hypothyroid', 'query hyperthyroid', 'lithium', 'goitre',
               'tumor', 'hypopituitary', 'psych', 'TSH measured',
               'T3 measured', 'TT4 measured', 'T4U measured',
               'FTI measured', 'TBG measured']

# Step 3: Map binary columns to 0 and 1
for col in binary_cols:
    df[col] = df[col].map({'t': 1, 'f': 0}).astype('int64')  # Ensure compatible dtype

# Encode categorical columns
df['sex'] = df['sex'].map({'M': 1, 'F': 0})
df['binaryClass'] = df['binaryClass'].map({'P': 1, 'N': 0})
label_encoder = LabelEncoder()
df['referral source'] = label_encoder.fit_transform(df['referral source'])

# Separate features and target
X = df.drop('binaryClass', axis=1)
y = df['binaryClass']

# Impute missing values
imputer = SimpleImputer(strategy='most_frequent')
X = imputer.fit_transform(X)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 2: Model Training
# Initialize models
models = {
    "XGBoost": XGBClassifier(random_state=42, eval_metric='logloss'),
}

# Train and evaluate models
best_model = None
best_score = 0
for model_name, model in models.items():
    print(f"\nTraining {model_name}...")
    start_time = time.time()
    model.fit(X_train, y_train)
    training_time = time.time() - start_time
    print(f"Training Time: {training_time:.2f} seconds")

# Evaluate the model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"{model_name} Performance:")
print(f"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}")

# Save the best model
if f1 > best_score:
    best_model = model
    best_score = f1

# Step 3: Model Evaluation
print("\nEvaluating the Best Model...")
y_pred = best_model.predict(X_test)
conf_matrix = confusion_matrix(y_test, y_pred)

print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", classification_report(y_test, y_pred))

# Visualize Feature Importances
if hasattr(best_model, 'feature_importances_'):
    feature_importances = best_model.feature_importances_
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(feature_importances)), feature_importances)
    plt.title('Feature Importances')
    plt.xlabel('Importance')
    plt.ylabel('Feature Index')
    plt.show()

# Step 4: Model Deployment
# Save the best model
joblib.dump(best_model, 'best_model.pkl')
print("Model saved as 'best_model.pkl'.")

# Deployment Instructions
print("To deploy the model as an API, use a framework like Flask or FastAPI.")
print("""
Example Flask API:
------------------
from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)
model = joblib.load('best_model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    prediction = model.predict([data['features']])
    return jsonify({'prediction': int(prediction[0])})

if __name__ == '__main__':
    app.run(debug=True)
""")





